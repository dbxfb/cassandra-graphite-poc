package com.criteo.sre.storage.sgrastar.singularity
package lucene

import com.google.common.cache.{Cache, CacheBuilder}
import java.nio.file.Path
import java.util.concurrent.ArrayBlockingQueue
import org.apache.lucene.analysis.standard.StandardAnalyzer
import org.apache.lucene.document.{Document, Field, LongField}
import org.apache.lucene.facet.{FacetField, FacetsCollector, FacetsConfig}
import org.apache.lucene.index._
import org.apache.lucene.search._
import org.apache.lucene.search.BooleanClause.Occur
import org.apache.lucene.store._
import org.slf4j.LoggerFactory
import scala.collection.JavaConversions._
import scala.collection.mutable.Queue
import scala.concurrent.{Await, Promise}
import scala.concurrent.duration.Duration
import scala.util.Try


/** Implements a metric index using Lucene as a backend.
  * The index can be queried using Graphite's metric patterns.
  *
  * We implement a lazy commit scheme to avoid writing to disk upon each write.
  */
class MetricsIndex(keyspace: String, indexPath: Option[Path], queueSize: Int = 100000) {
  private sealed trait Action
  private case class Insert(metric: Metric, token: Long) extends Action
  private case class Delete(metric: Metric) extends Action
  private case class Commit(promise: Promise[Unit]) extends Action
  private case object Shutdown extends Action

  private val log = LoggerFactory.getLogger(classOf[MetricsIndex])
  private val analyzer = new StandardAnalyzer()

  // Do not enable index compression, everything will only get slower.
  private val config = new IndexWriterConfig(analyzer)

  private val index = indexPath.map(FSDirectory.open).getOrElse(new RAMDirectory())
  private val writer = new IndexWriter(index, config)
  private var reader = DirectoryReader.open(writer, false)
  private var searcher = new IndexSearcher(reader)

  // TODO: softValues should be OK, but we may need a cache size limit
  private val queryCache: Cache[String, Query] =
    CacheBuilder
      .newBuilder()
      .softValues()
      .build()

  /** A commit threshold lower than 10k changes can cause the system to spend most of
    * its time committing in case of spikes (seen during synthetic benchmarking).
    */
  private var commitChangesThreshold: Long = 10000
  private var commitTimeThresholdMs: Long = 3 * 60 * 1000

  /** Using an ArrayBlockingQueue to avoid the extra garbage generated by linked lists.
    * We also get backpressure for free (slows down
    */
  private val actions = new ArrayBlockingQueue[Action](queueSize)
  private val updateThread = {
    val t = new Thread(new Runnable {def run() = updateLoop()})
    t.start
    t
  }

  def setCommitChangesThreshold(changes: Long): MetricsIndex = {
    commitChangesThreshold = changes
    this
  }

  def setCommitTimeThreshold(milliseconds: Long): MetricsIndex = {
    commitTimeThresholdMs = milliseconds
    this
  }

  def insert(m: Metric, token: Long) =
    // No checking on empty/invalid string because it should be done
    // at the time of data ingestion, not inside C*.
    actions.put(Insert(m, token))

  def delete(m: Metric) =
    actions.put(Delete(m))

  /** Looks for metrics which match the given Graphite search pattern and are contained
    * within the given token range.
    *
    * @param pattern Graphite metric pattern
    * @param from Lower token bound (exclusive)
    * @param to Upper token bound (inclusive)
    */
  def search(pattern: String, from: Long, to: Long): Seq[Metric] = {
    log.debug("{} - Searching for '{}' in ({}; {}]", keyspace, pattern, Long box from, Long box to)

    {
      Option(queryCache getIfPresent pattern)
    } orElse {
      buildAndCacheQuery(pattern)
    } map {
      rangedQueryWrapper(_, from, to)
    } map { query =>
      val collector = new ResultCollector()
      searcher.search(query, collector)

      collector
    } getOrElse {
      Seq()
    }
  }

  /** Computes the effective data size of the index in bytes.
    */
  def size(): Long = {
    var bytes: Long = 0

    // We use Try because of potential data races due to index compaction, which can delete files
    // after we obtained the list, causing NoSuchFileException upon trying to find the file's size.

    for (idxFile <- index.listAll)
      Try { bytes += index fileLength idxFile }

    bytes
  }

  def commit(synchronous: Boolean) = {
    val promise = Promise[Unit]()
    actions.put(Commit(promise))

    if (synchronous)
      Await.result(promise.future, Duration.Inf)
  }

  def close() = {
    log.info("{} - Closing index", keyspace)

    actions.put(Shutdown)
    updateThread.join()

    writer.close()
    index.close()
  }

  private def buildAndCacheQuery(pattern: String): Option[Query] = {
    patternToQuery(pattern).toOption map { query =>
      log.debug("{} - Generated query: {}", keyspace, query, "")

      val cachingQuery = new CachingWrapperQuery(query)
      queryCache.put(pattern, cachingQuery)

      cachingQuery
    }
  }

  private def buildAndCacheAutocompletionQuery(pattern: String): Query = {
    val parts = pattern split Metric.ElementSeparator
    val query = new BooleanQuery(true)

    // Go from most specific to most general (hence the reverse),
    // remove wildcard-only terms as they do not filter anything out
    for ((part, idx) <- parts.view.zipWithIndex.reverse.filterNot(_._1 == "**")) {
      val subQuery =
        if (part endsWith "**")
          new WildcardQuery(new Term(Metric.FieldPrefix + idx, part.substring(0, part.length - 1)))
        else
          new TermQuery(new Term(Metric.FieldPrefix + idx, part))

      query.add(subQuery, BooleanClause.Occur.MUST)
    }

    val cachingQuery = new CachingWrapperQuery(query)
    queryCache.put(pattern, cachingQuery)

    cachingQuery
  }

  private def rangedQueryWrapper(query: Query, from: Long, to: Long): Query = {
    val rangedQuery = new BooleanQuery(true)
    rangedQuery.add(query, BooleanClause.Occur.MUST)

    if (from < to) {
      // (fromExcl; toIncl]
      rangedQuery.add(
        NumericRangeQuery.newLongRange(Metric.FieldToken, from, to, false, true),
        BooleanClause.Occur.MUST
      )
    } else {
      // (fromExcl; MAX_LONG] || [MIN_LONG; toIncl]
      rangedQuery.add(
        NumericRangeQuery.newLongRange(Metric.FieldToken, from, Long.MaxValue, false, true),
        BooleanClause.Occur.MUST
      )
      rangedQuery.add(
        NumericRangeQuery.newLongRange(Metric.FieldToken, Long.MinValue, to, true, true),
        BooleanClause.Occur.MUST
      )
    }

    rangedQuery
  }

  private def updateLoop() = {
    var count: Long = 0
    var lastCommitTime: Long = System.currentTimeMillis
    var running = true
    while(running || !actions.isEmpty()) {
      actions.take() match {
        case Insert(metric, token) => {
          log.trace("{} - Inserting metric: {}", keyspace, metric, "")

          // Use updateDocument to ensure there are no duplicates.
          var doc = metric.toDocument
          doc.add(new LongField(Metric.FieldToken, token, Field.Store.NO))

          writer.updateDocument(metric.toTerm, doc)
          count += 1
        }
        case Delete(metric) => {
          log.trace("{} - Deleting metric: {}", keyspace, metric, "")
          writer.deleteDocuments(metric.toTerm)
          count += 1
        }
        case Commit(promise) => {
          persist()
          promise.success()
        }
        case Shutdown =>
          running = false
      }

      val timeMs = System.currentTimeMillis
      if (count > commitChangesThreshold || (timeMs - lastCommitTime) > commitTimeThresholdMs) {
        persist()

        count = 0
        lastCommitTime = timeMs
      }
    }

    // Make sure all the current changes have been written to disk
    persist()
  }

  private def persist() = {
    log.debug("{} - Committing changes to index", keyspace)
    writer.commit()

    // openIfChanged creates a new reader only when real changes happened,
    // and is able to re-use resources from the current reader.
    val newReader = DirectoryReader openIfChanged reader
    if (newReader != null) {
      reader = newReader
      searcher = new IndexSearcher(reader)
    }
  }


  /** Generates a Lucene query which matches the same metrics as the given
    * pattern (Graphite syntax).
    */
  private def patternToQuery(pattern: String): Try[Query] = Try {
    val parts = pattern split Metric.ElementSeparator
    var equal, wild, regex = Queue[Query]()

    // Remove wildcard-only terms as they do not filter anything out
    for ((part, i) <- parts.view.zipWithIndex.filterNot(_._1 == "*")) {
      if ((part contains '{') || (part contains '['))
        regex += new RegexpQuery(new Term(Metric.FieldPrefix + i, Metric elementToRegex part))
      else if ((part contains '*') || (part contains '?'))
        new WildcardQuery(new Term(Metric.FieldPrefix + i, part)) +=: wild
      else
        new TermQuery(new Term(Metric.FieldPrefix + i, part)) +=: equal
    }

    // Go from fastest to slowest, and from most specific to most general
    val query = new BooleanQuery(true)
    for (q <- equal.view ++ wild.view ++ regex.view)
      query.add(q, BooleanClause.Occur.MUST)

    // Filter on length as late as possible
    query.add(
      NumericRangeQuery.newIntRange(Metric.FieldLength, parts.length, parts.length, true, true),
      BooleanClause.Occur.MUST
    )

    query
  }
}
